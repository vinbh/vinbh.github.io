[{"content":"Demystifying the Unkillable Process in Linux —with a Little Help from Python TL;DR\nSIGKILL and SIGSTOP are meant to be final, but three kernel pathways let a task survive:\nThe process’s SIGNAL_UNKILLABLE flag is set (PID 1 has this by default). The thread is stuck in an uninterruptible sleep (D state) while inside the kernel. A tracer, cgroup freezer, or rogue kernel module silently drops or delays the signal. Below we’ll reproduce the first case with a 25‑line kernel module driven by Python, then learn to spot the other two cases with pure Python.\n1 Why SIGKILL ought to win In the Linux signal(7) manual, SIGKILL (9) and SIGSTOP (19) are marked “cannot be caught, blocked, or ignored.” Delivery can fail only when the caller lacks permission (kill() returns EPERM), or the target’s signal_struct→flags carries the bit SIGNAL_UNKILLABLE. The kernel grants that flag to the very first userspace task (PID 1) so the system can’t kill its own init.\n2 Flipping SIGNAL_UNKILLABLE: build an “immortal” process ⚠️ Run in a throw‑away VM or container.\nOnce a PID is flagged unkillable, only a reboot or voluntary exit clears it.\n2.1 The 25‑line kernel module unkillable.c 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // compile with: make \u0026amp;\u0026amp; sudo insmod unkillable.ko #include \u0026lt;linux/module.h\u0026gt; #include \u0026lt;linux/fs.h\u0026gt; #include \u0026lt;linux/pid.h\u0026gt; #include \u0026lt;linux/sched/signal.h\u0026gt; #define DEV \u0026#34;unkillable\u0026#34; static ssize_t flip(struct file *f, char __user *u, size_t pid, loff_t *o) { struct pid *p = find_get_pid(pid); if (p) { struct task_struct *t = pid_task(p, PIDTYPE_PID); if (t \u0026amp;\u0026amp; t-\u0026gt;signal) t-\u0026gt;signal-\u0026gt;flags |= SIGNAL_UNKILLABLE; /* 🔑 magic */ put_pid(p); } return 0; /* “read” zero bytes — side‑effect only */ } static const struct file_operations fops = { .read = flip }; static int __init init(void) { return register_chrdev(117, DEV, \u0026amp;fops); } static void __exit exit(void) { unregister_chrdev(117, DEV); } module_init(init); module_exit(exit); MODULE_LICENSE(\u0026#34;GPL\u0026#34;); Makefile\n1 2 3 4 5 obj-m += unkillable.o all: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules clean: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean Build \u0026amp; load:\n1 2 3 4 $ make $ sudo insmod unkillable.ko $ sudo mknod /dev/unkillable c 117 0 $ sudo chmod 666 /dev/unkillable 2.2 Python driver immortal.py 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/usr/bin/env python3 import os, time, ctypes pid = os.getpid() print(f\u0026#34;My PID is {pid}\u0026#34;) fd = os.open(\u0026#34;/dev/unkillable\u0026#34;, os.O_RDONLY) # read()’s *count* parameter is treated as the target PID by the driver ctypes.CDLL(None).read(fd, ctypes.c_char_p(0), pid) print(f\u0026#34;SIGNAL_UNKILLABLE flag set — hit me with `sudo kill -9 {pid}`\u0026#34;) while True: time.sleep(1) Run it:\n1 2 3 $ python3 immortal.py My PID is 44201 SIGNAL_UNKILLABLE flag set — hit me with `sudo kill -9 44201` 2.3 Shell test: kill -9 that fails 1 2 3 4 5 # second terminal $ sudo kill -9 44201 # exit‑status 0, signal accepted… $ ps -p 44201 -o pid,stat,cmd PID STAT CMD 44201 S python3 immortal.py # …but the process is still alive The kernel discarded the signal before delivery, so the task keeps running.\n3 Diagnosing stubborn PIDs with pure Python Most production “unkillables” aren’t flagged; they’re stuck inside the kernel.\n1 2 3 4 5 6 7 8 9 import psutil, signal for p in psutil.process_iter([\u0026#39;pid\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;status\u0026#39;]): if p.status() == psutil.STATUS_DISK_SLEEP: # ’D’ state print(\u0026#34;Blocked:\u0026#34;, p.pid, p.name()) try: p.send_signal(signal.SIGKILL) except psutil.AccessDenied: print(\u0026#34; EPERM — different UID or namespace?\u0026#34;) STATUS_DISK_SLEEP corresponds to kernel TASK_UNINTERRUPTIBLE.\nSIGKILL is queued but won’t run until the I/O finishes. psutil.AccessDenied (or kill -0 PID → EPERM) means you’re outside the target’s UID or PID namespace. 4 PID 1 quirks (host \u0026amp; containers) Global PID 1 is born with SIGNAL_UNKILLABLE; kill -9 1 returns EPERM. In containers, the entry‑point becomes PID 1 in that namespace and inherits the same immunity.\nFix: run your app under a mini‑init such as tini or dumb‑init so signals are forwarded and zombies reaped: 1 ENTRYPOINT [\u0026#34;tini\u0026#34;,\u0026#34;--\u0026#34;,\u0026#34;python\u0026#34;,\u0026#34;app.py\u0026#34;] 5 Better ways to pause or protect workloads Need Tool to use Why it’s better Pause/resume an entire workload cgroup freezer Stops tasks without abusing signals. Prevent accidental kills Supervisors (systemd, supervisord, Kubernetes) Let crashes happen, then auto‑restart. Faster memory cleanup post‑kill process_mrelease() (newer kernels) OOM reaper frees pages even if task is stuck. 6 Key take‑aways SIGKILL is absolute—unless the kernel never delivers it. Flipping SIGNAL_UNKILLABLE (or running as PID 1) is the only in‑kernel way to ignore kill -9.\n* The vast majority of “unkillable” sightings are really uninterruptible I/O or permission/namespace issues—no dark magic required. Happy hacking — and remember: with great CAP_SYS_MODULE comes great responsibility!\n","date":"2025-04-25T00:00:00Z","permalink":"https://vinbh.github.io/p/demystifying-the-unkillable-process-in-linux-with-a-little-help-from-python/","title":"Demystifying the *Unkillable* Process in Linux —with a Little Help from Python"},{"content":"Hey there! Welcome to my corner of the internet. I\u0026rsquo;m Vinbh, a tech enthusiast and avid gamer who loves to explore the intersection of technology and gaming. This blog is where I\u0026rsquo;ll share my adventures in both worlds - from debugging production issues to conquering virtual realms.\nWhat to Expect Here 🖥️ Tech Deep Dives: Exploring software engineering, DevOps practices, and system architecture 🎮 Gaming Adventures: Reviews, strategies, and stories from my gaming sessions 🛠️ Setup Guides: Optimizing both development and gaming environments 💡 Tips \u0026amp; Tricks: Useful shortcuts and life hacks for developers and gamers Stay tuned for regular updates on my latest projects, gaming achievements, and technical discoveries. Whether you\u0026rsquo;re here for the code or the games (or both!), I hope you\u0026rsquo;ll find something interesting.\nRemember: Always outnumbered, never outgunned! 🚀\n\u0026ldquo;In code we trust, in games we must.\u0026rdquo;\n","date":"2024-03-08T00:00:00Z","image":"https://vinbh.github.io/p/hello-world/cover_hu_b57928d9c9c6b244.jpg","permalink":"https://vinbh.github.io/p/hello-world/","title":"Welcome to My Digital Playground"},{"content":"Why Are There Only 13 DNS Root Server Addresses? Introduction If you’re reading this blog post, chances are you’re already reaping the benefits of a highly distributed system for resolving domain names. Every time you type in a website address or click a link, the Domain Name System (DNS) springs into action, translating your friendly “www” addresses into numerical IP addresses. But you might have come across a puzzling fact: there are only 13 DNS root server addresses. Let’s explore why that is and bust a common misconception!\n**“Why are there only 13 DNS root server addresses?\nA common misconception is that there are only 13 root servers in the world. In reality there are many more, but still only 13 IP addresses used to query the different root server networks. Limitations in the original architecture of DNS require there to be a maximum of 13 server addresses in the root zone. In the early days of the Internet, there was only one server for each of the 13 IP addresses, most of which were located in the United States.\nToday each of the 13 IP addresses has several servers, which use Anycast routing to distribute requests based on load and proximity. Right now there are over 600 different DNS root servers distributed across every populated continent on earth.”**\nThe Myth of the 13 Servers One of the most common internet myths is that there are only 13 physical DNS root servers worldwide. Imagine if that were true—nearly the entire planet’s DNS lookups would be handled by a mere handful of machines! That could be a bit scary, like having only 13 vending machines for coffee for everyone on Earth. (We’d never get caffeinated enough!)\nIn truth, the number 13 corresponds to 13 unique IP addresses, not 13 actual physical servers.\nThe Historical Reason When DNS was first developed, its architecture was limited in how many name server addresses could be listed in the root zone. The engineers decided on a maximum of 13, due to technical constraints related to:\nProtocol Limitations: Early DNS packets had size limitations, affecting how many root server entries could be included. Network Efficiency: DNS was originally designed for a smaller internet, not the mega-network we use today. Anycast Magic Fast-forward to the modern era, and we have Anycast routing to save the day. The idea behind Anycast is delightfully simple yet highly effective:\nMultiple Servers, One IP: You have multiple servers around the globe, but each shares the same IP address. Geographical Proximity: Internet traffic is routed automatically to the nearest or least busy server using this shared IP. Load Balancing: The load is spread among many servers, increasing reliability and speed. Thanks to Anycast, each of those 13 “root server addresses” can represent a cluster of physical servers scattered across multiple continents. As of now, there are over 600 physically distinct servers operating under those 13 addresses, ensuring global coverage and robust DNS resolution.\nWhy This Matters Resilience: With so many distributed servers, DNS remains stable even if some servers go down. Speed: You’re usually routed to the nearest root server, which means faster website resolutions. Scalability: More servers can always be added to the clusters under the same IP to handle increased global internet usage. Fun Fact You might stumble upon root servers named with letters—like A, B, C, etc. These labels correspond to each of the 13 IP addresses. For example, “A” is one of the addresses (named a.root-servers.net), and it has multiple physical servers worldwide.\nFinal Thoughts So, the next time you hear someone say there are only 13 DNS root servers, feel free to put on your DNS superhero cape and explain the real story. The “13” refers to IP addresses, not the number of physical machines! In reality, these IP addresses direct you to hundreds of actual servers via the Anycast wonder.\nIf you’re ever curious about where your nearest root server is located, there are DNS tools out there to show your DNS route. It’s a neat exercise in seeing how globally connected the internet really is—even behind the scenes.\n","date":"2025-03-10T00:00:00Z","permalink":"https://vinbh.github.io/p/the-myth-of-the-13-dns-root-server-addresses/","title":"The Myth of the 13 DNS Root Server Addresses"},{"content":"I still remember my first encounter with TCP/IP back in the early 2014. Trying to debug why my game was lagging, I stumbled upon a world of packets, acknowledgments, and sequence numbers that seemed impenetrable at first. Years later, I\u0026rsquo;ve come to appreciate the elegant dance that happens beneath our everyday internet experience. Let me guide you through it.\nBeyond the Buzzwords: TCP and IP Unwrapped When we talk about \u0026ldquo;TCP/IP,\u0026rdquo; we\u0026rsquo;re really discussing two distinct protocols working in tandem. IP (Internet Protocol) handles the addressing and routing—essentially determining where packets should go. TCP (Transmission Control Protocol) ensures reliability, handling the how of data transmission.\nIP is like the postal service\u0026rsquo;s infrastructure—addresses, sorting facilities, and delivery routes. TCP is more like certified mail with tracking, receipt confirmation, and guaranteed delivery. One without the other leaves you with either a reliable system that can\u0026rsquo;t find its destination or excellent routing with no guarantees of delivery.\nThe Famous Three-Way Handshake Before a single byte of your cat video or important business document traverses the internet, TCP performs an elaborate greeting ritual known as the three-way handshake.\nYour Browser Web Server | | | SYN (seq=42) | | --------------------------→ | \u0026quot;Hello, I'd like to talk. | | My reference number is 42.\u0026quot; | | | SYN-ACK (seq=100,ack=43) | | ←--------------------------- | \u0026quot;I hear you! Your ref is 42+1, | | mine is 100.\u0026quot; | | | ACK (ack=101) | | --------------------------→ | \u0026quot;Got it! Let's start talking!\u0026quot; | | Connection Established Connection Established\nWhat\u0026rsquo;s fascinating here isn\u0026rsquo;t just the mechanical exchange, but the implied vulnerability. When your device sends that initial SYN packet, it allocates memory and resources in anticipation of the connection. This became the basis for the infamous SYN flood attacks that brought down major websites in the late 1990s—attackers would send thousands of SYN packets with no intention of completing the handshake, exhausting server resources.\nTCP\u0026rsquo;s Cautious Congestion Control One aspect of TCP that continues to amaze me is its inherent caution. Unlike humans who often dive headfirst into situations, TCP approaches network capacity with remarkable restraint through its slow-start mechanism.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ┌─────────────────┐ │ Connection │ │ Starts │ └────────┬────────┘ ▼ ┌─────────────────┐ │ Initial cwnd = │ │ 10 segments │ └────────┬────────┘ ▼ ┌─────────────────┐ No ┌─────────────────┐ │ Acknowledgments ├────────────────────► Timeout, Reset │ │ Received? │ └────────┬────────┘ └────────┬────────┘ │ │ Yes │ ▼ │ ┌─────────────────┐ │ │ Double cwnd │ │ └────────┬────────┘ │ ▼ │ ┌─────────────────┐ │ │ Packet Loss │ Yes │ │ Detected? ├─────────────────┐ │ └────────┬────────┘ │ │ │ No ▼ │ │ ┌─────────────────┐ │ └──────────────────► Cut cwnd in half├─┘ └─────────────────┘ I once debugged a strange performance issue where file transfers would start slowly and then suddenly accelerate after a few seconds. The culprit? TCP\u0026rsquo;s slow-start algorithm doing exactly what it should—cautiously probing the network\u0026rsquo;s capacity before ramping up.\nInitial congestion window sizes have evolved over time. The original TCP specifications suggested starting with just 1 segment, but modern implementations typically use 10 segments (about 14KB). This evolution reflects our changing networks—from the fragile early internet to today\u0026rsquo;s robust infrastructure.\nThe Throughput Equation Nobody Tells You About Here\u0026rsquo;s something I rarely see discussed outside academic papers: the TCP throughput is fundamentally limited by an equation relating packet loss, round-trip time, and maximum segment size:\n1 Max Throughput ≈ (MSS/RTT) * (1/√p) Where:\nMSS = Maximum Segment Size RTT = Round Trip Time p = Packet loss probability This equation shocked me when I first encountered it. A mere 0.1% packet loss can dramatically limit throughput on high-latency connections. This is why your video call to Australia stutters even with a \u0026ldquo;fast\u0026rdquo; internet connection—physics and mathematics conspire against you.\nTCP Fast Open: Skipping the Formalities Anyone who\u0026rsquo;s worked with high-frequency API calls knows the pain of TCP connection overhead. TCP Fast Open (TFO) addresses this by allowing data transmission during the initial handshake.\n1 2 3 4 5 6 7 8 9 10 11 12 Client Server | | | SYN + TFO Cookie + DATA | | ---------------------------→ | | | | SYN-ACK + ACK(DATA) + DATA | | ←--------------------------- | | | | ACK | | ---------------------------→ | | | | Data exchange already started! I\u0026rsquo;ve seen this reduce page load times by 10-15% for API-heavy applications—not revolutionary, but those milliseconds add up to a noticeably smoother user experience.\nThe Practical Side: TCP Tuning Tools After years of wrestling with network performance, I\u0026rsquo;ve accumulated a toolkit for TCP diagnosis and tuning:\n1 2 3 4 5 6 7 8 # See your current TCP settings sysctl net.ipv4.tcp_* # Watch TCP connections in real-time ss -tunap # Capture and analyze TCP flows tcpdump -i eth0 -nn \u0026#39;tcp port 80\u0026#39; -w capture.pcap Modern Linux distributions have sane defaults, but in specific scenarios (high-bandwidth, high-latency links), tweaking parameters like tcp_rmem and tcp_wmem can yield significant improvements. I once doubled throughput on a transpacific link just by adjusting these buffers.\nWhen TCP Shows Its Age Despite its elegance, TCP was designed in a different era. Its conservative approach can be detrimental in certain scenarios:\nMobile networks with rapidly changing conditions confuse TCP\u0026rsquo;s congestion algorithms Short-lived connections (like API calls) barely escape slow-start before terminating High bandwidth-delay product paths struggle to utilize available capacity This is why Google developed QUIC (which evolved into HTTP/3), employing UDP as a foundation and reimplementing reliability mechanisms with modern networks in mind.\nConclusion: The Invisible Orchestra What fascinates me most about TCP/IP isn\u0026rsquo;t just its technical intricacies, but how it embodies certain values: caution, fairness, reliability, and adaptability. When billions of devices run these protocols, they create an invisible orchestra of give-and-take that allows our global network to function.\nNext time your browser loads a page, picture those SYN packets setting off on their journey, the careful dance of slow-start packets testing the network\u0026rsquo;s limits, and the congestion avoidance algorithms ensuring everyone gets their fair share of the pipe.\nUnderstanding TCP/IP isn\u0026rsquo;t just technical knowledge—it\u0026rsquo;s appreciating the digital social contract that makes our connected world possible.\nDo you have questions about TCP/IP or network performance? Drop a comment below—I\u0026rsquo;m always up for a good networking discussion!\nReference: For more in-depth details, please refer to Chapter 2 of High Performance Browser Networking by Ilya Grigorik.\n","date":"2024-03-09T00:00:00Z","permalink":"https://vinbh.github.io/p/the-choreography-of-packets-how-tcp/ip-actually-works/","title":"The Choreography of Packets: How TCP/IP Actually Works"},{"content":"Avoiding Scalability Collapse in Lock-Heavy Systems: Lessons for SREs As Site Reliability Engineers (SREs), we often deal with scaling distributed systems, optimizing performance, and ensuring high availability. One of the subtle yet critical challenges in highly concurrent environments is scalability collapse due to saturated locks. A recent study (Dice \u0026amp; Kogan, 2019) sheds light on how lock contention can lead to sudden performance degradation and proposes Generic Concurrency Restriction (GCR) as a mitigation strategy.\nThe Problem: When More Threads Hurt Performance In a multi-core system, locks ensure exclusive access to shared resources. However, as the number of threads waiting for a lock increases, the performance of the application can fade or drop abruptly. This phenomenon, known as scalability collapse, happens because:\nThreads compete for shared resources (e.g., CPU cache, last-level cache (LLC)). Increased cache misses and contention lead to performance degradation. More threads waiting for a lock waste CPU cycles, exacerbating the slowdown. Real-World Example: Microservices and Database Locks Imagine an SRE managing a high-traffic microservices architecture where multiple services interact with a database. If a critical section (e.g., updating a shared counter) is protected by a lock, high concurrency can cause:\nIncreased contention on the lock. Threads waiting longer to acquire the lock, reducing throughput. Potential CPU starvation, leading to cascading failures. Similar issues can arise in load balancers, rate-limiting mechanisms, and leader election processes.\nThe Solution: Generic Concurrency Restriction (GCR) The paper introduces Generic Concurrency Restriction (GCR), a lock-agnostic mechanism that intercepts lock acquisition calls and decides when a thread is allowed to proceed. This avoids excessive contention and improves overall system performance.\nKey Benefits of GCR: Reduces contention by limiting the number of threads acquiring a lock. Enhances NUMA awareness (in GCR-NUMA) by ensuring threads running on the same socket acquire the lock. Introduces negligible overhead when the lock is uncontended. Improves performance by orders of magnitude in contention-heavy scenarios. SRE Best Practices to Avoid Scalability Collapse While GCR is a promising approach, SREs should consider the following strategies to mitigate lock contention issues:\n1. Monitor and Profile Lock Contention Use tools like:\neBPF-based tracers (e.g., bcc, perf, LockStat in Java) Prometheus metrics (process_thread_cpu_time_seconds, thread_blocked_time) Flame graphs to identify lock-heavy functions 2. Use Adaptive Concurrency Control Instead of blindly increasing worker threads:\nImplement load shedding (e.g., dropping requests instead of queuing). Use adaptive thread pools that scale based on system metrics. Apply backpressure mechanisms to prevent thread explosion. 3. Prefer Lock-Free or Optimistic Concurrency Techniques Where possible:\nUse lock-free data structures (e.g., ConcurrentHashMap, CAS-based algorithms). Leverage optimistic concurrency control (OCC) over pessimistic locking. Consider event-driven architectures instead of synchronous locking. 4. Optimize for NUMA Awareness Pin threads to specific NUMA nodes to reduce remote memory access overhead. Use NUMA-aware memory allocation to improve cache efficiency. 5. Mitigate Oversubscription in Cloud Deployments Avoid overcommitting CPU resources in Kubernetes (requests vs. limits). Use cgroup limits to prevent one container from monopolizing CPU. Enable thread-aware scaling policies in autoscalers (HPA/VPA). Conclusion Lock contention is a hidden performance killer that can cripple the scalability of even well-architected systems. GCR and NUMA-aware locking strategies provide effective ways to manage concurrency without sacrificing throughput. As SREs, our role is to observe, measure, and adapt—ensuring that our systems scale gracefully under high load.\nBy integrating concurrency-aware monitoring, adaptive thread control, and NUMA optimizations, we can prevent scalability collapse and build highly resilient distributed systems.\n📌 Have you faced lock contention issues in production? Share your experiences in the PRs!\n","date":"2025-03-15T00:00:00Z","permalink":"https://vinbh.github.io/p/breaking-the-lock-how-sres-can-prevent-scalability-collapse-and-keep-systems-blazing-fast/","title":"Breaking the Lock: How SREs Can Prevent Scalability Collapse and Keep Systems Blazing Fast"},{"content":"Gaming Highlights Epic Gaming Moments Some PUBG PC fun!!\nGame Reviews \u0026amp; First Impressions FarCry4 Yeti vs 50BMG!\nTech Content ","date":"2024-03-08T00:00:00Z","permalink":"https://vinbh.github.io/p/video-gallery/","title":"Video Gallery"},{"content":"Latest Gaming Adventures I\u0026rsquo;ve been diving deep into some incredible games lately. Whether it\u0026rsquo;s mastering raid mechanics or exploring vast open worlds, gaming has always been my perfect balance to coding sessions.\nCurrent Favorites Exploring the depths of Palworld\u0026rsquo;s crafting system Competitive matches in Valorant Building automation systems in Factorio Currently Playing Red Dead Redemption 2: Riding the frontier and rewriting my own legend. Age of Empires III: Commanding ancient armies like a modern-day general. Assassin\u0026rsquo;s Creed Odyssey: Embarking on mythic quests through ancient worlds. Sniper Elite: Resistance: Sharpening my sniper skills with every covert mission. Tech Projects \u0026amp; Learnings Current Stack I\u0026rsquo;m currently working with:\nBackend: Go, Python Frontend: Duh! DevOps: Docker, Kubernetes, o11y Cloud: AWS Recent Discoveries Found some amazing tools that have revolutionized my workflow:\nNeovim for lightning-fast code editing Docker Compose for local development GitHub Copilot for pair programming with AI Weekly Tech Tips Here\u0026rsquo;s a useful Git trick I discovered recently:\n1 git config --global alias.undo \u0026#39;reset --soft HEAD~1\u0026#39; Now you can undo your last commit with git undo!\nGaming Setup My current battle station:\nCPU: AMD Ryzen 9 3900x GPU: NVIDIA RTX 4090 RAM: 64GB DDR4 Display: 27\u0026quot; 1440p 144Hz and 4K OLED 55\u0026quot; What\u0026rsquo;s Next? Stay tuned for:\nDeep dives into system design Game performance optimization guides Weekly coding challenges Gaming achievement guides Photo by Codioful on Unsplash\n","date":"2023-08-25T00:00:00Z","image":"https://vinbh.github.io/p/my-tech-gaming-journey/cover_hu_1a644060dac854f1.jpg","permalink":"https://vinbh.github.io/p/my-tech-gaming-journey/","title":"My Tech \u0026 Gaming Journey"}]