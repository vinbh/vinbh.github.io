<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tech on vinbh</title><link>https://vinbh.github.io/tags/tech/</link><description>Recent content in Tech on vinbh</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 10 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://vinbh.github.io/tags/tech/index.xml" rel="self" type="application/rss+xml"/><item><title>The Myth of the 13 DNS Root Server Addresses</title><link>https://vinbh.github.io/p/the-myth-of-the-13-dns-root-server-addresses/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://vinbh.github.io/p/the-myth-of-the-13-dns-root-server-addresses/</guid><description>&lt;h1 id="why-are-there-only-13-dns-root-server-addresses">&lt;a href="#why-are-there-only-13-dns-root-server-addresses" class="header-anchor">&lt;/a>Why Are There Only 13 DNS Root Server Addresses?
&lt;/h1>&lt;h2 id="introduction">&lt;a href="#introduction" class="header-anchor">&lt;/a>Introduction
&lt;/h2>&lt;p>If you’re reading this blog post, chances are you’re already reaping the benefits of a highly distributed system for resolving domain names. Every time you type in a website address or click a link, the Domain Name System (DNS) springs into action, translating your friendly “www” addresses into numerical IP addresses. But you might have come across a puzzling fact: &lt;strong>there are only 13 DNS root server addresses.&lt;/strong> Let’s explore why that is and bust a common misconception!&lt;/p>
&lt;blockquote>
&lt;p>**“Why are there only 13 DNS root server addresses?&lt;br>
A common misconception is that there are only 13 root servers in the world. In reality there are many more, but still only 13 IP addresses used to query the different root server networks. Limitations in the original architecture of DNS require there to be a maximum of 13 server addresses in the root zone. In the early days of the Internet, there was only one server for each of the 13 IP addresses, most of which were located in the United States.&lt;/p>
&lt;p>Today each of the 13 IP addresses has several servers, which use Anycast routing to distribute requests based on load and proximity. Right now there are over 600 different DNS root servers distributed across every populated continent on earth.”**&lt;/p>&lt;/blockquote>
&lt;h2 id="the-myth-of-the-13-servers">&lt;a href="#the-myth-of-the-13-servers" class="header-anchor">&lt;/a>The Myth of the 13 Servers
&lt;/h2>&lt;p>One of the most common internet myths is that there are only 13 &lt;em>physical&lt;/em> DNS root servers worldwide. Imagine if that were true—nearly the entire planet’s DNS lookups would be handled by a mere handful of machines! That could be a bit scary, like having only 13 vending machines for coffee for everyone on Earth. (We’d never get caffeinated enough!)&lt;/p>
&lt;p>In truth, the number 13 corresponds to &lt;strong>13 unique IP addresses&lt;/strong>, not 13 actual physical servers.&lt;/p>
&lt;h2 id="the-historical-reason">&lt;a href="#the-historical-reason" class="header-anchor">&lt;/a>The Historical Reason
&lt;/h2>&lt;p>When DNS was first developed, its architecture was limited in how many name server addresses could be listed in the root zone. The engineers decided on a maximum of 13, due to technical constraints related to:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Protocol Limitations:&lt;/strong> Early DNS packets had size limitations, affecting how many root server entries could be included.&lt;/li>
&lt;li>&lt;strong>Network Efficiency:&lt;/strong> DNS was originally designed for a smaller internet, not the mega-network we use today.&lt;/li>
&lt;/ol>
&lt;h2 id="anycast-magic">&lt;a href="#anycast-magic" class="header-anchor">&lt;/a>Anycast Magic
&lt;/h2>&lt;p>Fast-forward to the modern era, and we have &lt;strong>Anycast routing&lt;/strong> to save the day. The idea behind Anycast is delightfully simple yet highly effective:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Multiple Servers, One IP&lt;/strong>: You have multiple servers around the globe, but each shares the &lt;em>same&lt;/em> IP address.&lt;/li>
&lt;li>&lt;strong>Geographical Proximity&lt;/strong>: Internet traffic is routed automatically to the nearest or least busy server using this shared IP.&lt;/li>
&lt;li>&lt;strong>Load Balancing&lt;/strong>: The load is spread among many servers, increasing reliability and speed.&lt;/li>
&lt;/ul>
&lt;p>Thanks to Anycast, each of those 13 “root server addresses” can represent a cluster of physical servers scattered across multiple continents. As of now, there are &lt;strong>over 600&lt;/strong> physically distinct servers operating under those 13 addresses, ensuring global coverage and robust DNS resolution.&lt;/p>
&lt;h2 id="why-this-matters">&lt;a href="#why-this-matters" class="header-anchor">&lt;/a>Why This Matters
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>Resilience&lt;/strong>: With so many distributed servers, DNS remains stable even if some servers go down.&lt;/li>
&lt;li>&lt;strong>Speed&lt;/strong>: You’re usually routed to the nearest root server, which means faster website resolutions.&lt;/li>
&lt;li>&lt;strong>Scalability&lt;/strong>: More servers can always be added to the clusters under the same IP to handle increased global internet usage.&lt;/li>
&lt;/ul>
&lt;h2 id="fun-fact">&lt;a href="#fun-fact" class="header-anchor">&lt;/a>Fun Fact
&lt;/h2>&lt;p>You might stumble upon root servers named with letters—like &lt;code>A&lt;/code>, &lt;code>B&lt;/code>, &lt;code>C&lt;/code>, etc. These labels correspond to each of the 13 IP addresses. For example, “A” is one of the addresses (named &lt;code>a.root-servers.net&lt;/code>), and it has multiple physical servers worldwide.&lt;/p>
&lt;h2 id="final-thoughts">&lt;a href="#final-thoughts" class="header-anchor">&lt;/a>Final Thoughts
&lt;/h2>&lt;p>So, the next time you hear someone say there are only 13 DNS root servers, feel free to put on your DNS superhero cape and explain the real story. &lt;strong>The “13” refers to IP addresses, not the number of physical machines!&lt;/strong> In reality, these IP addresses direct you to hundreds of actual servers via the Anycast wonder.&lt;/p>
&lt;p>If you’re ever curious about where your nearest root server is located, there are DNS tools out there to show your DNS route. It’s a neat exercise in seeing how globally connected the internet really is—even behind the scenes.&lt;/p>
&lt;hr></description></item><item><title>The Choreography of Packets: How TCP/IP Actually Works</title><link>https://vinbh.github.io/p/the-choreography-of-packets-how-tcp/ip-actually-works/</link><pubDate>Sat, 09 Mar 2024 00:00:00 +0000</pubDate><guid>https://vinbh.github.io/p/the-choreography-of-packets-how-tcp/ip-actually-works/</guid><description>&lt;p>I still remember my first encounter with TCP/IP back in the early 2014. Trying to debug why my game was lagging, I stumbled upon a world of packets, acknowledgments, and sequence numbers that seemed impenetrable at first. Years later, I&amp;rsquo;ve come to appreciate the elegant dance that happens beneath our everyday internet experience. Let me guide you through it.&lt;/p>
&lt;h2 id="beyond-the-buzzwords-tcp-and-ip-unwrapped">&lt;a href="#beyond-the-buzzwords-tcp-and-ip-unwrapped" class="header-anchor">&lt;/a>Beyond the Buzzwords: TCP and IP Unwrapped
&lt;/h2>&lt;p>When we talk about &amp;ldquo;TCP/IP,&amp;rdquo; we&amp;rsquo;re really discussing two distinct protocols working in tandem. IP (Internet Protocol) handles the addressing and routing—essentially determining &lt;em>where&lt;/em> packets should go. TCP (Transmission Control Protocol) ensures reliability, handling the &lt;em>how&lt;/em> of data transmission.&lt;/p>
&lt;p>IP is like the postal service&amp;rsquo;s infrastructure—addresses, sorting facilities, and delivery routes. TCP is more like certified mail with tracking, receipt confirmation, and guaranteed delivery. One without the other leaves you with either a reliable system that can&amp;rsquo;t find its destination or excellent routing with no guarantees of delivery.&lt;/p>
&lt;h2 id="the-famous-three-way-handshake">&lt;a href="#the-famous-three-way-handshake" class="header-anchor">&lt;/a>The Famous Three-Way Handshake
&lt;/h2>&lt;p>Before a single byte of your cat video or important business document traverses the internet, TCP performs an elaborate greeting ritual known as the three-way handshake.&lt;/p>
&lt;pre>&lt;code>Your Browser Web Server
| |
| SYN (seq=42) |
| --------------------------→ | &amp;quot;Hello, I'd like to talk.
| | My reference number is 42.&amp;quot;
| |
| SYN-ACK (seq=100,ack=43) |
| ←--------------------------- | &amp;quot;I hear you! Your ref is 42+1,
| | mine is 100.&amp;quot;
| |
| ACK (ack=101) |
| --------------------------→ | &amp;quot;Got it! Let's start talking!&amp;quot;
| |
&lt;/code>&lt;/pre>
&lt;p>Connection Established Connection Established&lt;/p>
&lt;p>What&amp;rsquo;s fascinating here isn&amp;rsquo;t just the mechanical exchange, but the implied vulnerability. When your device sends that initial SYN packet, it allocates memory and resources in anticipation of the connection. This became the basis for the infamous SYN flood attacks that brought down major websites in the late 1990s—attackers would send thousands of SYN packets with no intention of completing the handshake, exhausting server resources.&lt;/p>
&lt;h2 id="tcps-cautious-congestion-control">&lt;a href="#tcps-cautious-congestion-control" class="header-anchor">&lt;/a>TCP&amp;rsquo;s Cautious Congestion Control
&lt;/h2>&lt;p>One aspect of TCP that continues to amaze me is its inherent caution. Unlike humans who often dive headfirst into situations, TCP approaches network capacity with remarkable restraint through its slow-start mechanism.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> ┌─────────────────┐
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Connection │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Starts │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └────────┬────────┘
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ▼
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ┌─────────────────┐
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Initial cwnd = │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ 10 segments │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └────────┬────────┘
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ▼
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ┌─────────────────┐ No ┌─────────────────┐
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Acknowledgments ├────────────────────► Timeout, Reset │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Received? │ └────────┬────────┘
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └────────┬────────┘ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Yes │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ▼ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ┌─────────────────┐ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Double cwnd │ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └────────┬────────┘ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ▼ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ┌─────────────────┐ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Packet Loss │ Yes │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ Detected? ├─────────────────┐ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └────────┬────────┘ │ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ No ▼ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> │ ┌─────────────────┐ │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └──────────────────► Cut cwnd in half├─┘
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> └─────────────────┘
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>I once debugged a strange performance issue where file transfers would start slowly and then suddenly accelerate after a few seconds. The culprit? TCP&amp;rsquo;s slow-start algorithm doing exactly what it should—cautiously probing the network&amp;rsquo;s capacity before ramping up.&lt;/p>
&lt;p>Initial congestion window sizes have evolved over time. The original TCP specifications suggested starting with just 1 segment, but modern implementations typically use 10 segments (about 14KB). This evolution reflects our changing networks—from the fragile early internet to today&amp;rsquo;s robust infrastructure.&lt;/p>
&lt;h2 id="the-throughput-equation-nobody-tells-you-about">&lt;a href="#the-throughput-equation-nobody-tells-you-about" class="header-anchor">&lt;/a>The Throughput Equation Nobody Tells You About
&lt;/h2>&lt;p>Here&amp;rsquo;s something I rarely see discussed outside academic papers: the TCP throughput is fundamentally limited by an equation relating packet loss, round-trip time, and maximum segment size:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Max Throughput ≈ (MSS/RTT) * (1/√p)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>MSS = Maximum Segment Size&lt;/li>
&lt;li>RTT = Round Trip Time&lt;/li>
&lt;li>p = Packet loss probability&lt;/li>
&lt;/ul>
&lt;p>This equation shocked me when I first encountered it. A mere 0.1% packet loss can dramatically limit throughput on high-latency connections. This is why your video call to Australia stutters even with a &amp;ldquo;fast&amp;rdquo; internet connection—physics and mathematics conspire against you.&lt;/p>
&lt;h2 id="tcp-fast-open-skipping-the-formalities">&lt;a href="#tcp-fast-open-skipping-the-formalities" class="header-anchor">&lt;/a>TCP Fast Open: Skipping the Formalities
&lt;/h2>&lt;p>Anyone who&amp;rsquo;s worked with high-frequency API calls knows the pain of TCP connection overhead. TCP Fast Open (TFO) addresses this by allowing data transmission during the initial handshake.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> Client Server
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | SYN + TFO Cookie + DATA |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | ---------------------------→ |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | SYN-ACK + ACK(DATA) + DATA |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | ←--------------------------- |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | ACK |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | ---------------------------→ |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | |
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> | Data exchange already started!
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>I&amp;rsquo;ve seen this reduce page load times by 10-15% for API-heavy applications—not revolutionary, but those milliseconds add up to a noticeably smoother user experience.&lt;/p>
&lt;h2 id="the-practical-side-tcp-tuning-tools">&lt;a href="#the-practical-side-tcp-tuning-tools" class="header-anchor">&lt;/a>The Practical Side: TCP Tuning Tools
&lt;/h2>&lt;p>After years of wrestling with network performance, I&amp;rsquo;ve accumulated a toolkit for TCP diagnosis and tuning:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># See your current TCP settings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">sysctl net.ipv4.tcp_*
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Watch TCP connections in real-time&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ss -tunap
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Capture and analyze TCP flows&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">tcpdump -i eth0 -nn &lt;span class="s1">&amp;#39;tcp port 80&amp;#39;&lt;/span> -w capture.pcap
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Modern Linux distributions have sane defaults, but in specific scenarios (high-bandwidth, high-latency links), tweaking parameters like &lt;code>tcp_rmem&lt;/code> and &lt;code>tcp_wmem&lt;/code> can yield significant improvements. I once doubled throughput on a transpacific link just by adjusting these buffers.&lt;/p>
&lt;h2 id="when-tcp-shows-its-age">&lt;a href="#when-tcp-shows-its-age" class="header-anchor">&lt;/a>When TCP Shows Its Age
&lt;/h2>&lt;p>Despite its elegance, TCP was designed in a different era. Its conservative approach can be detrimental in certain scenarios:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Mobile networks&lt;/strong> with rapidly changing conditions confuse TCP&amp;rsquo;s congestion algorithms&lt;/li>
&lt;li>&lt;strong>Short-lived connections&lt;/strong> (like API calls) barely escape slow-start before terminating&lt;/li>
&lt;li>&lt;strong>High bandwidth-delay product paths&lt;/strong> struggle to utilize available capacity&lt;/li>
&lt;/ol>
&lt;p>This is why Google developed QUIC (which evolved into HTTP/3), employing UDP as a foundation and reimplementing reliability mechanisms with modern networks in mind.&lt;/p>
&lt;h2 id="conclusion-the-invisible-orchestra">&lt;a href="#conclusion-the-invisible-orchestra" class="header-anchor">&lt;/a>Conclusion: The Invisible Orchestra
&lt;/h2>&lt;p>What fascinates me most about TCP/IP isn&amp;rsquo;t just its technical intricacies, but how it embodies certain values: caution, fairness, reliability, and adaptability. When billions of devices run these protocols, they create an invisible orchestra of give-and-take that allows our global network to function.&lt;/p>
&lt;p>Next time your browser loads a page, picture those SYN packets setting off on their journey, the careful dance of slow-start packets testing the network&amp;rsquo;s limits, and the congestion avoidance algorithms ensuring everyone gets their fair share of the pipe.&lt;/p>
&lt;p>Understanding TCP/IP isn&amp;rsquo;t just technical knowledge—it&amp;rsquo;s appreciating the digital social contract that makes our connected world possible.&lt;/p>
&lt;hr>
&lt;p>&lt;em>Do you have questions about TCP/IP or network performance? Drop a comment below—I&amp;rsquo;m always up for a good networking discussion!&lt;/em>&lt;/p>
&lt;blockquote>
&lt;p>Reference: For more in-depth details, please refer to Chapter 2 of &lt;em>High Performance Browser Networking&lt;/em> by Ilya Grigorik.&lt;/p>&lt;/blockquote></description></item></channel></rss>